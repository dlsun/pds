{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of 10.5 Regular Expression and Sentiment Analysis.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UYXoMwEpG40",
        "colab_type": "text"
      },
      "source": [
        "# 10.5 Regular Expression and Sentiment Analysis\n",
        "\n",
        "Sentiment analysis is the use of natural language processing to quantify subjective information. Our goal in this section is to use machine learning to identify whether a piece of text captures positive, negative, or neutral emotions. Sentiment analysis has become more prevalent in our world through its application in algorithmic traders, recommendation systems, and market research.  "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2vgbSDB4v5rF",
        "colab_type": "text"
      },
      "source": [
        "## Sentiment Analysis\n",
        "\n",
        "Consider the following sentences:\n",
        "\n",
        "1. \"I am so happy to be here right now!\"\n",
        "2. \"I'm pretty sad about this whole thing.\"\n",
        "\n",
        "Most people would agree that the first sentence exhibits positive emotion and the second sentence exhibits negative emotion. We perceive it to be this way because the first sentence has the word happy and the second sentence has the word sad. With this very simple idea in mind, we can build a very na√Øve classifier that determines if a sentence exhibits positive, negative, or neutral emotion. \n",
        "\n",
        "Our output being a range between -1 and 1 with sentence towards -1 as having negative sentiment and sentices towards +1 having positive sentiment.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5tcQToKM0eBY",
        "colab_type": "code",
        "outputId": "8f297650-236b-475d-d25c-992a963b8a4b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "positive_words = set([\"happy\", \"great\", \"fanstastic\", \"love\", \"appreciate\", \"grateful\"])\n",
        "negative_words = set([\"sad\", \"gross\", \"disturbing\", \"bitter\", \"sorry\", \"pathetic\"])\n",
        "\n",
        "def sentiment_analyzer_v2(sentence): \n",
        "    sentence = sentence.lower().split(\" \")\n",
        "\n",
        "    pos_word_cnt = 0\n",
        "    neg_word_cnt = 0\n",
        "\n",
        "    for word in sentence: \n",
        "        if word in positive_words: \n",
        "            pos_word_cnt += 1\n",
        "        elif word in negative_words: \n",
        "            neg_word_cnt += 1\n",
        "    \n",
        "    return (pos_word_cnt - neg_word_cnt) / (pos_word_cnt + neg_word_cnt)\n",
        "        \n",
        "print(sentiment_analyzer_v2(\"This is making me happy !\"))\n",
        "print(sentiment_analyzer_v2(\"This is making me sad !\"))\n",
        "print(sentiment_analyzer_v2(\"I am neither happy nor sad .\"))"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "1.0\n",
            "-1.0\n",
            "0.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gk2g-nvq1-Xf",
        "colab_type": "text"
      },
      "source": [
        "Given a big enough dictionary of positive and negative words, this algorithm can work pretty well. But it takes a lot of work to figure out what words are happy and then type it into a list, its just not efficient. So, being the clever Data Scientists that we are, let's create a machine learning algorithm. \n",
        "\n",
        "Here is the schematics: let's get a list of texts that are labelled as either positive, negative, or netural. We use a count vector to see what words occured in which text and how many times that word occured. We then use a machine learning algorithm to train on this count vector along with the sentiment label. In essense, this algorithm is saying \"if these words occured $n$ number of times in a text, then it is likely for it to be a specific sentiment\"\n",
        "\n",
        "Download a set of tweets from (link) and you will see that there are two features that we care about: polarity and text. Polarity tells us what the sentiment is (0 for negative, 2 for neutral, 4 for positive). To keep things consistent with our naive algorithm above, let's map the polarity such that positive is +1, neutral is 0, and negative is -1. With this encoding, the closer our prediction is to 1, the more positive the sentiment is and the closer our prediction is to -1, the more negative the sentiment is. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d1Vy9AYt4F3-",
        "colab_type": "code",
        "outputId": "a6c0836d-268a-4fda-9983-5dc182601e16",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "import pandas as pd\n",
        "\n",
        "df_tweets = pd.read_csv(\"https://raw.githubusercontent.com/bfkwong/data/master/twitter_sentiment.csv\")\n",
        "df_tweets[\"polarity\"] = df_tweets[\"polarity\"].map({4:1, 2:0, 0:-1})\n",
        "df_tweets.head()"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>tpryan</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>vcu451</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>chadfu</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>SIX15</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>yamarama</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                               text\n",
              "0         1  ...  @stellargirl I loooooooovvvvvveee my Kindle2. ...\n",
              "1         1  ...  Reading my kindle2...  Love it... Lee childs i...\n",
              "2         1  ...  Ok, first assesment of the #kindle2 ...it fuck...\n",
              "3         1  ...  @kenburbary You'll love your Kindle2. I've had...\n",
              "4         1  ...  @mikefish  Fair enough. But i have the Kindle2...\n",
              "\n",
              "[5 rows x 6 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zIFResqfRiOt",
        "colab_type": "text"
      },
      "source": [
        "### Text Normalization\n",
        "\n",
        "The goal of normalizing is to remove excess noise so that the algorithm only has to focus on what is important. Think of this process as the text version of `StandardScaler`.\n",
        "\n",
        "**Lemmatization** is one popular normalization technique. During lemmatization, the words `studies` and `studying` gets lemmatized to `study`. In essense, the process of lemmatization turns different forms of the same word (i.e. studies, studying) into the same base lemma (study). This helps reduce the noise in our dataset. \n",
        "\n",
        "**Stop word removal** is another way to normalize our text in order to reduce noise. Stop words such as \"there\", \"how\", \"then\", \"we\" offer no additional clues for deciding what the sentiment of a sentence is. Thus, it makes sense for us to remove these words before training out algorithm \n",
        "\n",
        "Lemmatization and stop word removal are both tedious tasks for us to do, which is why NLTK provides us with functions to remove them. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tloDK32sSxNt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 306
        },
        "outputId": "bb0d1c6d-14a7-42c9-959e-2be71e324325"
      },
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem.wordnet import WordNetLemmatizer\n",
        "\n",
        "stop_words=set(stopwords.words(\"english\"))\n",
        "tweets = list(df_tweets[\"text\"])\n",
        "for tweet in range(len(tweets)): \n",
        "    tweets[tweet] = [x for x in word_tokenize(tweets[tweet]) if x not in stop_words]\n",
        "\n",
        "lem = WordNetLemmatizer()\n",
        "for tweet in range(len(tweets)): \n",
        "    tweets[tweet] = [lem.lemmatize(x) for x in tweets[tweet]]\n",
        "\n",
        "df_tweets[\"processed_text\"] = [\" \".join(x) for x in tweets]\n",
        "df_tweets.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>polarity</th>\n",
              "      <th>tweet_id</th>\n",
              "      <th>tweet_date</th>\n",
              "      <th>query</th>\n",
              "      <th>user</th>\n",
              "      <th>text</th>\n",
              "      <th>processed_text</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>Mon May 11 03:17:40 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>tpryan</td>\n",
              "      <td>@stellargirl I loooooooovvvvvveee my Kindle2. ...</td>\n",
              "      <td>@ stellargirl I loooooooovvvvvveee Kindle2 . N...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>4</td>\n",
              "      <td>Mon May 11 03:18:03 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>vcu451</td>\n",
              "      <td>Reading my kindle2...  Love it... Lee childs i...</td>\n",
              "      <td>Reading kindle2 ... Love ... Lee child good re...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>5</td>\n",
              "      <td>Mon May 11 03:18:54 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>chadfu</td>\n",
              "      <td>Ok, first assesment of the #kindle2 ...it fuck...</td>\n",
              "      <td>Ok , first assesment # kindle2 ... fucking roc...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "      <td>Mon May 11 03:19:04 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>SIX15</td>\n",
              "      <td>@kenburbary You'll love your Kindle2. I've had...</td>\n",
              "      <td>@ kenburbary You 'll love Kindle2 . I 've mine...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1</td>\n",
              "      <td>7</td>\n",
              "      <td>Mon May 11 03:21:41 UTC 2009</td>\n",
              "      <td>kindle2</td>\n",
              "      <td>yamarama</td>\n",
              "      <td>@mikefish  Fair enough. But i have the Kindle2...</td>\n",
              "      <td>@ mikefish Fair enough . But Kindle2 I think '...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   polarity  ...                                     processed_text\n",
              "0         1  ...  @ stellargirl I loooooooovvvvvveee Kindle2 . N...\n",
              "1         1  ...  Reading kindle2 ... Love ... Lee child good re...\n",
              "2         1  ...  Ok , first assesment # kindle2 ... fucking roc...\n",
              "3         1  ...  @ kenburbary You 'll love Kindle2 . I 've mine...\n",
              "4         1  ...  @ mikefish Fair enough . But Kindle2 I think '...\n",
              "\n",
              "[5 rows x 7 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zMUQylZN4JQC",
        "colab_type": "text"
      },
      "source": [
        "With this data, let's use the CountVectorizer to turn the tweets into a collection of words. To make sure we exclude any hashtags and @ symbols, we will also specify a regular expression tokenizer to only include alphabet characters with the `tokenizer` parameter.\n",
        "\n",
        "Additionally, we want to specify `ngram_range = (1,2)`. This is to help provide context to the words. Consider the double negative string `I do not dislike` which carries positive sentiment. If we split the words into unigrams, we get words like `not` and `dislike`, which are negative words. By using a bigram, we are able to train the algorithm to realize that `not dislike` is actually a positive term. Thus, allowing the algorithm to be able to handle difficult to decipher sentiments like double negatives and sarcasm. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "25ZsIkbos6gR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from nltk.tokenize import RegexpTokenizer\n",
        "\n",
        "token = RegexpTokenizer(r'[a-zA-Z]+')\n",
        "cv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,2),tokenizer = token.tokenize)\n",
        "tweet_text_cv = cv.fit_transform(df_tweets['processed_text'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mSjQ6Pms4Ang",
        "colab_type": "text"
      },
      "source": [
        "Why are we doing this? Given that we have a label for whether the piece of text exhibits positive, negative, or neutral emotions, we can use the count vector to see what words tend to occur in positive sentences, and etc. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I9h-lDkt5QAb",
        "colab_type": "code",
        "outputId": "15b3389d-78b5-4c80-e6bf-f3eb13fc43c2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        }
      },
      "source": [
        "vocab = [[x, cv.vocabulary_[x]] for x in cv.vocabulary_]\n",
        "vocab.sort(key=lambda x:x[1])\n",
        "vocab = [x[0] for x in vocab]\n",
        "\n",
        "df_twitter_cv = pd.DataFrame(tweet_text_cv.todense(), columns=vocab)\n",
        "df_twitter_cv[\"polarity\"] = df_tweets[\"polarity\"]\n",
        "\n",
        "df_twitter_cv.head()"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>aapl</th>\n",
              "      <th>aapl es</th>\n",
              "      <th>abortion</th>\n",
              "      <th>abortion zealot</th>\n",
              "      <th>absolutely</th>\n",
              "      <th>absolutely blow</th>\n",
              "      <th>absolutely hilarious</th>\n",
              "      <th>accannis</th>\n",
              "      <th>accannis edog</th>\n",
              "      <th>access</th>\n",
              "      <th>access damn</th>\n",
              "      <th>access throttle</th>\n",
              "      <th>accident</th>\n",
              "      <th>accident guess</th>\n",
              "      <th>accident location</th>\n",
              "      <th>according</th>\n",
              "      <th>according create</th>\n",
              "      <th>accosts</th>\n",
              "      <th>accosts roger</th>\n",
              "      <th>account</th>\n",
              "      <th>account request</th>\n",
              "      <th>acg</th>\n",
              "      <th>acg custom</th>\n",
              "      <th>aching</th>\n",
              "      <th>acia</th>\n",
              "      <th>acia pills</th>\n",
              "      <th>actually</th>\n",
              "      <th>actually quite</th>\n",
              "      <th>ad</th>\n",
              "      <th>ad adobe</th>\n",
              "      <th>ad w</th>\n",
              "      <th>adam</th>\n",
              "      <th>adam lambert</th>\n",
              "      <th>add</th>\n",
              "      <th>add people</th>\n",
              "      <th>addiction</th>\n",
              "      <th>addiction thank</th>\n",
              "      <th>addictive</th>\n",
              "      <th>adidas</th>\n",
              "      <th>adidas billups</th>\n",
              "      <th>...</th>\n",
              "      <th>years great</th>\n",
              "      <th>yeeeee</th>\n",
              "      <th>yeezy</th>\n",
              "      <th>yeezy khaki</th>\n",
              "      <th>yema</th>\n",
              "      <th>yes</th>\n",
              "      <th>yes gm</th>\n",
              "      <th>yes lol</th>\n",
              "      <th>yes m</th>\n",
              "      <th>yes video</th>\n",
              "      <th>yesterday</th>\n",
              "      <th>yesterday cbs</th>\n",
              "      <th>yk</th>\n",
              "      <th>yo</th>\n",
              "      <th>yo teach</th>\n",
              "      <th>york</th>\n",
              "      <th>york times</th>\n",
              "      <th>youtube</th>\n",
              "      <th>youtube adobe</th>\n",
              "      <th>yr</th>\n",
              "      <th>yr old</th>\n",
              "      <th>ytz</th>\n",
              "      <th>yuan</th>\n",
              "      <th>yuan invested</th>\n",
              "      <th>yummmmmy</th>\n",
              "      <th>zealot</th>\n",
              "      <th>zealot n</th>\n",
              "      <th>zero</th>\n",
              "      <th>zero desire</th>\n",
              "      <th>zet</th>\n",
              "      <th>zet o</th>\n",
              "      <th>zic</th>\n",
              "      <th>zlff</th>\n",
              "      <th>zomg</th>\n",
              "      <th>zomg g</th>\n",
              "      <th>zoom</th>\n",
              "      <th>zoom lebron</th>\n",
              "      <th>zydrunas</th>\n",
              "      <th>zydrunas awesome</th>\n",
              "      <th>polarity</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5 rows √ó 5483 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   aapl  aapl es  abortion  ...  zydrunas  zydrunas awesome  polarity\n",
              "0     0        0         0  ...         0                 0         1\n",
              "1     0        0         0  ...         0                 0         1\n",
              "2     0        0         0  ...         0                 0         1\n",
              "3     0        0         0  ...         0                 0         1\n",
              "4     0        0         0  ...         0                 0         1\n",
              "\n",
              "[5 rows x 5483 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 40
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qRecjyTw7iB0",
        "colab_type": "text"
      },
      "source": [
        "If a certain word occurs very frequently in texts that are labeled as positive texts, then we can make the assumption that the word is positive. So if in the future we encounter a sentence with this word, we should classify the sentence as positive. \n",
        "\n",
        "With this idea in mind, let's train a model to predict whether a sentence exhibits positive, negative, or neutral emotions. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5MQxI6NTtkQ6",
        "colab_type": "code",
        "outputId": "d5cd83c2-9d91-44ac-c4ca-a2806be56931",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn import metrics\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(tweet_text_cv, \n",
        "                                                    df_tweets['polarity'], \n",
        "                                                    test_size=0.3, \n",
        "                                                    random_state=1)\n",
        "\n",
        "sentiment_analyzer = LinearRegression().fit(X_train, y_train)\n",
        "predicted = sentiment_analyzer.predict(X_test)\n",
        "print(\"LinearRegression R^2:\\t\", sentiment_analyzer.score(X_test, y_test))\n",
        "print(\"LinearRegression MSE:\\t\",metrics.mean_squared_error(y_test, predicted))"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "LinearRegression R^2:\t 0.3904061882859056\n",
            "LinearRegression MSE:\t 0.43156532563528044\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "336lT6cO9C5A",
        "colab_type": "text"
      },
      "source": [
        "Let's see this bad boy in action. Consider the following sentences. The model was able to correctly classify the sentence has having positive leaning polarity"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "truyzLGe8iB6",
        "colab_type": "code",
        "outputId": "58d6be48-f315-400b-cad3-15b1de6e870f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Clearly positive sentence\n",
        "test = cv.transform([\"I love being here!\"]).todense()\n",
        "sentiment_analyzer.predict(test)"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.23681886])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QeZ66XQUUj5A",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "759a34ab-8f7d-4606-e8d3-911d219fe465"
      },
      "source": [
        "# Clearly negative sentence\n",
        "test = cv.transform([\"I hate this.\"]).todense()\n",
        "sentiment_analyzer.predict(test)"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.41152944])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QnY1HqkAVH09",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49e33736-cd9a-4090-af71-fee58fb0fa5c"
      },
      "source": [
        "# Ambiguous positive sentence with double negative\n",
        "test = cv.transform([\"I do not dislike school.\"]).todense()\n",
        "sentiment_analyzer.predict(test)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.05733004])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYD55k4NXLSp",
        "colab_type": "text"
      },
      "source": [
        "Since we used a linear model in this, we can analyze the coefficients of each variable to see what contributes most to positive and negative sentiment. Recall that each variable represents either an ngram, the ngram that is the biggest is the `most positive` and the ngram that is the smallest is the `most negative`. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zXUbPfHOYb9o",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "outputId": "859752b3-6459-416e-f604-499074f36d5a"
      },
      "source": [
        "ngrams = [x for x in df_twitter_cv.columns if x != \"polarity\"]\n",
        "\n",
        "df_word_coef = pd.DataFrame([ngrams,sentiment_analyzer.coef_], index=[\"word\", \"coef\"]).T.set_index(\"word\")\n",
        "df_word_coef.sort_values(\"coef\", ascending=False)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>coef</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>word</th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>loves twitter</th>\n",
              "      <td>0.441781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loves</th>\n",
              "      <td>0.441781</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>g</th>\n",
              "      <td>0.404112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>cool</th>\n",
              "      <td>0.387307</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>loved</th>\n",
              "      <td>0.38585</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>gm</th>\n",
              "      <td>-0.389355</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>comcast</th>\n",
              "      <td>-0.390205</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fighting</th>\n",
              "      <td>-0.406231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>fighting latex</th>\n",
              "      <td>-0.406231</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>hate</th>\n",
              "      <td>-0.421888</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>5482 rows √ó 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "                    coef\n",
              "word                    \n",
              "loves twitter   0.441781\n",
              "loves           0.441781\n",
              "g               0.404112\n",
              "cool            0.387307\n",
              "loved            0.38585\n",
              "...                  ...\n",
              "gm             -0.389355\n",
              "comcast        -0.390205\n",
              "fighting       -0.406231\n",
              "fighting latex -0.406231\n",
              "hate           -0.421888\n",
              "\n",
              "[5482 rows x 1 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-ivL_T4qa16K",
        "colab_type": "text"
      },
      "source": [
        "As expected, words like `hate` and `fight` has very negative connotations to it while words like `loves` and `cool` has very positive connotation to it. Another thing we can look at is the intercept, which tells us the overall sentiment of the entire training corpus. As you can see below, the overall sentinment of the training corpus is rather neutral."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rocJ7syaYd82",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "49a88a1a-c8b6-4e89-c529-049eecd3a8a6"
      },
      "source": [
        "sentiment_analyzer.intercept_"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.01035837109566326"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4iu58XS1bjc0",
        "colab_type": "text"
      },
      "source": [
        "## NLTK Implementation \n",
        "\n",
        "This is a lot of tedious work, and whenever there is a lot of tedious work, you can bet that there's a library for that. The following is the NLTK sentiment analysis algorithm using a very similar technique as we implemented above. \n",
        "\n",
        "NLTK is a different library than SciKit-Learn so it will require us to do our preprocessing a bit differently. The remainder of the section will walk you through the differences."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yzphuVSGrUIr",
        "colab_type": "text"
      },
      "source": [
        "### Text Preprocessing\n",
        "\n",
        "NLTK requires that your training examples are in the form of a list of tuples with 2 elements where the first element are word tokens and the second element is the class. An example would be: \n",
        "\n",
        "```\n",
        "[([\"I\", \"love\", \"Pepsi\"], 1), ([\"I\", \"am\", \"not\", \"a\", \"fan\", \"of\", \"Coke\"], -1), ...]\n",
        "```\n",
        "\n",
        "The following code creates this encoding as well as creating a train test split:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KSz0heZnmATQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 425
        },
        "outputId": "1d597d9d-7b74-4b8b-a0ca-a8a924ec7c42"
      },
      "source": [
        "from nltk.tokenize import TweetTokenizer\n",
        "from nltk.classify import NaiveBayesClassifier\n",
        "from nltk.sentiment import SentimentAnalyzer\n",
        "from nltk.sentiment.util import *\n",
        "import random\n",
        "\n",
        "tweet_tknize = TweetTokenizer()\n",
        "df_tweetsnltk = df_tweets.copy()[[\"polarity\", \"processed_text\"]]\n",
        "\n",
        "polarity_score = df_tweetsnltk.polarity\n",
        "tokenized_tweets = list(df_tweetsnltk.processed_text.apply(tweet_tknize.tokenize))\n",
        "\n",
        "tweets_formatted = []\n",
        "for x in range(len(polarity_score)):\n",
        "    tweets_formatted.append((tokenized_tweets[x], polarity_score[x]))\n",
        "\n",
        "training_tweets = tweets_formatted[:int(0.70 * len(tweets_formatted))]\n",
        "testing_tweets = tweets_formatted[int(0.70 * len(tweets_formatted)):]\n",
        "\n",
        "tweets[0:2]"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[['@',\n",
              "  'stellargirl',\n",
              "  'I',\n",
              "  'loooooooovvvvvveee',\n",
              "  'Kindle2',\n",
              "  '.',\n",
              "  'Not',\n",
              "  'DX',\n",
              "  'cool',\n",
              "  ',',\n",
              "  '2',\n",
              "  'fantastic',\n",
              "  'right',\n",
              "  '.'],\n",
              " ['Reading',\n",
              "  'kindle2',\n",
              "  '...',\n",
              "  'Love',\n",
              "  '...',\n",
              "  'Lee',\n",
              "  'child',\n",
              "  'good',\n",
              "  'read',\n",
              "  '.']]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "znirIYg1srFx",
        "colab_type": "text"
      },
      "source": [
        "### Feature Extraction\n",
        "\n",
        "Now, that we have the tokenize string and its labels. Let's create our `SentimentAnalyzer` object and extract unigrams to prepare the tweets for training: "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hIByTveaenDj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Create our SentimentAnalyzer\n",
        "sentim_analyzer = SentimentAnalyzer()\n",
        "\n",
        "# Get all words/tokens that is in our trianing set\n",
        "# This formats our data in the way that the next function requires it\n",
        "all_words = sentim_analyzer.all_words(training_tweets)\n",
        "\n",
        "# Get the formatted all_words list and create unigrams out of it\n",
        "unigram_feats = sentim_analyzer.unigram_word_feats(all_words, min_freq=4)\n",
        "\n",
        "# We add this feature to our SentimentAnalyzer object\n",
        "sentim_analyzer.add_feat_extractor(extract_unigram_feats, unigrams=unigram_feats)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ISXRyMlfvMji",
        "colab_type": "text"
      },
      "source": [
        "### Training our Sentiment Analyzer\n",
        "\n",
        "We will be using a NaiveBayesClassifer for this instance. NLTK only supports classifiers for sentiment analysis"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICZ4jrtcsUZd",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "ba1e989e-2e4a-4560-8ebd-486d181619ee"
      },
      "source": [
        "training_set = sentim_analyzer.apply_features(training_tweets)\n",
        "trainer = NaiveBayesClassifier.train\n",
        "classifier = sentim_analyzer.train(trainer, training_set)"
      ],
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training classifier\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZLNMDM8vaun",
        "colab_type": "text"
      },
      "source": [
        "### Testing our Sentiment Analyzer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wigp271ivLsI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "outputId": "e39420fe-1650-4f94-f9f0-0f7a858f4f9f"
      },
      "source": [
        "testing_set = sentim_analyzer.apply_features(testing_tweets)\n",
        "sorted(sentim_analyzer.evaluate(testing_set).items())"
      ],
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Evaluating NaiveBayesClassifier results...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Accuracy', 0.6333333333333333),\n",
              " ('F-measure [-1]', 0.6837606837606838),\n",
              " ('F-measure [0]', 0.6304347826086957),\n",
              " ('F-measure [1]', 0.5714285714285714),\n",
              " ('Precision [-1]', 0.625),\n",
              " ('Precision [0]', 0.8055555555555556),\n",
              " ('Precision [1]', 0.52),\n",
              " ('Recall [-1]', 0.7547169811320755),\n",
              " ('Recall [0]', 0.5178571428571429),\n",
              " ('Recall [1]', 0.6341463414634146)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rUFp7fSwvvD7",
        "colab_type": "text"
      },
      "source": [
        "Now that we know it works, let's test it with some random tweets we pulled from Twitter."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AptkeGbNvcbC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "b709bf50-b3f1-4e87-b2ea-0dd815693f98"
      },
      "source": [
        "# Clearly positive sentence\n",
        "sentim_analyzer.classify(\"I love being here!\".split(\" \"))"
      ],
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j8jolufOwzVy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "f430a625-7430-4598-8448-acc1c286a163"
      },
      "source": [
        "# Clearly negative sentence\n",
        "sentim_analyzer.classify(\"I hate this.\".split(\" \"))"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "-1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2j7DvCPIyG3s",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "095aff26-2077-49e4-e14c-bdfb1622965d"
      },
      "source": [
        "# Ambiguous positive sentence with double negative\n",
        "sentim_analyzer.classify(\"I do not dislike school.\".split(\" \"))"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXaLsyo7y4Z3",
        "colab_type": "text"
      },
      "source": [
        "# Exercises"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x5ANpEW3y5Mx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}